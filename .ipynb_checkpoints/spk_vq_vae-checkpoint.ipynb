{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# for vanilla VQ, uncomment next line and comment out next next line\n",
    "# from model.model_v2 import spk_vq_vae_resnet\n",
    "from model.model_v2_EWA import spk_vq_vae_resnet\n",
    "from model.utils import Helper\n",
    "\n",
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% parameters passed to data loader\n",
    "import collections\n",
    "args = collections.namedtuple\n",
    "\n",
    "# args.train_data_path = './data/wave_clus_noise01_c4.mat'\n",
    "# args.test_data_path = './data/wave_clus_noise01_c4.mat'\n",
    "\n",
    "# args.train_data_path = './data/hc1.mat'\n",
    "# args.test_data_path = './data/hc1.mat'\n",
    "\n",
    "# args.train_data_path = './data/C_drift.mat'\n",
    "# args.test_data_path = './data/C_drift.mat'\n",
    "\n",
    "# args.train_data_path = './data/pac_scream70dB_spks_3d.mat'\n",
    "# args.test_data_path = './data/pac_scream70dB_spks_3d.mat'\n",
    "\n",
    "args.train_data_path = '../DeepVAE_data/C_difficult1_spks_c4.mat'\n",
    "args.test_data_path = '../DeepVAE_data/C_difficult1_spks_c4.mat'\n",
    "\n",
    "# args.train_data_path = './data/C_easy1_spks_c4.mat'\n",
    "# args.test_data_path = './data/C_easy1_spks_c4.mat'\n",
    "\n",
    "# args.train_data_path = '../DeepVAE_data/Neuropixels_spks_c4.mat'\n",
    "# args.test_data_path = '../DeepVAE_data/Neuropixels_spks_c4.mat'\n",
    "\n",
    "args.train_ratio = .6\n",
    "args.test_ratio = .4\n",
    "args.seed = 1\n",
    "args.batch_size = 48\n",
    "args.test_batch_size = 128\n",
    "args.randperm = True\n",
    "\n",
    "if args.train_data_path is not args.test_data_path:\n",
    "    args.val_norm = True\n",
    "else:\n",
    "    args.val_norm = False\n",
    "\n",
    "# %% global parameters\n",
    "spk_ch = 4\n",
    "args.spk_ch = spk_ch\n",
    "spk_dim = 64\n",
    "log_interval = 50\n",
    "beta = 0.15\n",
    "vq_num = 128\n",
    "\n",
    "\"\"\"\n",
    "org_dim     = param[0]\n",
    "conv1_ch    = param[1]\n",
    "conv2_ch    = param[2]\n",
    "conv1_ker   = param[4]\n",
    "conv2_ker   = param[5]\n",
    "self.vq_dim = param[6]\n",
    "self.vq_num = param[7]\n",
    "cardinality = param[8]\n",
    "vq on chan  = param[9]\n",
    "\"\"\"\n",
    "param_resnet_v2 = [spk_ch, 256, 16, 1, 3, 1, int(spk_dim/4), vq_num, 16, 4, False, 0.2]\n",
    "\n",
    "# %% train/test splitting and normalization\n",
    "helper = Helper(args)\n",
    "train_loader, test_loader = helper.create_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "model = spk_vq_vae_resnet(param_resnet_v2).to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def loss_function(recon_x, x, commit_loss):\n",
    "    recon_loss = F.mse_loss(recon_x, x, size_average=False)\n",
    "    return recon_loss + beta * commit_loss, recon_loss\n",
    "    #return recon_loss + vq_loss, recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4, amsgrad=True)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-6, weight_decay=1e-5, momentum=0.9)\n",
    "#optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-6)\n",
    "#optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=.9)\n",
    "\n",
    "# decay_embed, decay_rest = [], []\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'embed' in name:\n",
    "#         decay_embed.append(param)\n",
    "#     else:\n",
    "#         decay_rest.append(param)\n",
    "\n",
    "# optimizer = optim.SGD([\n",
    "#     {'params': decay_rest, 'weight_decay':1e-4},\n",
    "#     {'params': decay_embed, 'weight_decay':1e-5, 'momentum':0.9}\n",
    "#     ], lr=1e-9, momentum=0.9)\n",
    "\n",
    "# scheduler = MultiStepLR(optimizer, milestones=[250,350,600], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        #data = Variable(data).cuda()\n",
    "        data = data.to(gpu)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, commit_loss = model(data)\n",
    "        loss, recon_loss = loss_function(recon_batch, data, commit_loss)\n",
    "        loss.backward(retain_graph=True)\n",
    "        model.bwd()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #train_loss += recon_loss.data[0] * len(data)\n",
    "        train_loss += recon_loss.item() / (spk_dim * spk_ch)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), recon_loss.item()))\n",
    "\n",
    "    #print(model.embed.weight.data)\n",
    "    \n",
    "    average_train_loss = train_loss / len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average train loss: {:.5f}'.format(\n",
    "          epoch, average_train_loss))\n",
    "    return average_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% save model\n",
    "best_val_loss = 10 # v3\n",
    "cur_train_loss = 1\n",
    "#best_val_loss = .3965 # v1\n",
    "def save_model(val_loss, train_loss):\n",
    "\tglobal best_val_loss, cur_train_loss\n",
    "\tif val_loss < best_val_loss:\n",
    "\t\tbest_val_loss = val_loss\n",
    "\t\tcur_train_loss = train_loss\n",
    "\t\ttorch.save(model.state_dict(), './spk_vq_vae_temp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    model.embed_reset()\n",
    "    test_loss = 0\n",
    "    recon_sig = torch.rand(1, spk_ch, spk_dim)\n",
    "    org_sig = torch.rand(1, spk_ch, spk_dim)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data = data.to(gpu)\n",
    "\n",
    "            recon_batch, commit_loss = model(data)\n",
    "            _, recon_loss = loss_function(recon_batch, data, commit_loss)\n",
    "        \n",
    "            recon_sig = torch.cat((recon_sig, recon_batch.data.cpu()), dim=0)\n",
    "            org_sig = torch.cat((org_sig, data.data.cpu()), dim=0)\n",
    "        \n",
    "            #test_loss += recon_loss.data[0] * len(data)\n",
    "            test_loss += recon_loss.item() / (spk_dim * spk_ch)\n",
    "\n",
    "        average_test_loss = test_loss / len(test_loader.dataset)\n",
    "        print('====> Epoch: {} Average test loss: {:.5f}'.format(\n",
    "              epoch, average_test_loss))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.bar(np.arange(vq_num), model.embed_freq / model.embed_freq.sum())\n",
    "        plt.show()\n",
    "\n",
    "    return average_test_loss, recon_sig[1:], org_sig[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% training and validating\n",
    "train_loss_history = []\n",
    "test_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    #scheduler.step()\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "    test_loss, _, _ = test(epoch)\n",
    "    #save_model(test_loss, train_loss)\n",
    "    \n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Minimal train/testing losses are {:.4f} and {:.4f} with index {}\\n'\n",
    "    .format(cur_train_loss, best_val_loss, test_loss_history.index(min(test_loss_history))))\n",
    "\n",
    "# plot train and test loss against epochs\n",
    "plt.figure(1)\n",
    "epoch_axis = range(1, len(train_loss_history) + 1)\n",
    "plt.plot(epoch_axis, train_loss_history, 'bo')\n",
    "plt.plot(epoch_axis, test_loss_history, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# n_row, n_col = 8, 8\n",
    "# f, axarr = plt.subplots(n_row, n_col, figsize=(n_col*1.5, n_row*1.5))\n",
    "# cur_ker = model.embed.weight.data.cpu().numpy()\n",
    "# for i in range(n_row):\n",
    "#     for j in range(n_col):\n",
    "#         axarr[i, j].plot(cur_ker[i*n_row+j], 'r')\n",
    "#         axarr[i, j].axis('off')\n",
    "# plt.show()\n",
    "\n",
    "#model.load_state_dict(torch.load('./spk_vq_vae_temp.pt'))\n",
    "#torch.save(model.state_dict(), './cae_models/spk_vq_vae_hc1_vq{}_N{}.pt'.format(vq_num, param_resnet_v2[2]))\n",
    "#torch.save(model.state_dict(), './cae_models/spk_vq_vae_neuropixels_c15r_vq{}_N{}.pt'.format(vq_num, param_resnet_v2[2]))\n",
    "#torch.save(model.state_dict(), './cae_models/spk_vq_vae_waveclus_vq{}_N{}.pt'.format(vq_num, param_resnet_v2[2]))\n",
    "\n",
    "embed_idx = np.argsort(model.embed_freq)\n",
    "embed_sort = model.dict_val.data.cpu().numpy()[embed_idx]\n",
    "\n",
    "plt.figure()\n",
    "n_row, n_col = 1, 8\n",
    "f, axarr = plt.subplots(n_row, n_col, figsize=(n_col*2, n_row*2))\n",
    "for i in range(8):\n",
    "    axarr[i].plot(embed_sort[i], 'r')\n",
    "    axarr[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% spike recon\n",
    "train_mean, train_std, _ = helper.param_for_recon()\n",
    "train_mean, train_std = torch.from_numpy(train_mean), torch.from_numpy(train_std)\n",
    "_, val_spks, test_spks = test(9)\n",
    "\n",
    "# val_spks = val_spks.numpy()\n",
    "# test_spks = test_spks.numpy()\n",
    "\n",
    "# calculate compression ratio\n",
    "vq_freq = model.embed_freq / sum(model.embed_freq)\n",
    "vq_freq = vq_freq[vq_freq != 0]\n",
    "vq_log2 = np.log2(vq_freq)\n",
    "bits = -sum(np.multiply(vq_freq, vq_log2))\n",
    "cr = spk_ch * spk_dim * 16 / (param_resnet_v2[2] * bits)\n",
    "print('compression ratio is {:.2f} with {:.2f}-bit.'.format(cr, bits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recon_spks = (val_spks * train_std + train_mean).view(-1, spk_dim)\n",
    "test_spks_v2 = (test_spks * train_std + train_mean).view(-1, spk_dim)\n",
    "\n",
    "recon_err = torch.norm(recon_spks-test_spks_v2, p=2, dim=1) / torch.norm(test_spks_v2, p=2, dim=1)\n",
    "\n",
    "print('mean of recon_err is {:.4f}'.format(torch.mean(recon_err)))\n",
    "print('std of recon_err is {:.4f}'.format(torch.std(recon_err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% spike visualization for 3-d inputs\n",
    "recon_spks_new = recon_spks.numpy()\n",
    "test_spks_new = test_spks_v2.numpy()\n",
    "\n",
    "def cal_sndr(org_data, recon_data):\n",
    "    org_norm = np.linalg.norm(org_data, axis=1)\n",
    "    err_norm = np.linalg.norm(org_data-recon_data, axis=1)\n",
    "    return np.mean(20*np.log10(org_norm / err_norm)), np.std(20*np.log10(org_norm / err_norm))\n",
    "\n",
    "cur_sndr, sndr_std = cal_sndr(test_spks_new, recon_spks_new)\n",
    "print('SNDR is {:.4f} with std {:.4f}'.format(cur_sndr, sndr_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_val_idx = np.random.permutation(len(recon_spks_new))\n",
    "\n",
    "plt.figure()\n",
    "n_row, n_col = 3, 8\n",
    "spks_to_show = test_spks_new[rand_val_idx[:n_row*n_col]]\n",
    "ymax, ymin = np.amax(spks_to_show), np.amin(spks_to_show)\n",
    "f, axarr = plt.subplots(n_row, n_col, figsize=(n_col*3, n_row*3))\n",
    "for i in range(n_row):\n",
    "    for j in range(n_col):\n",
    "        axarr[i, j].plot(recon_spks_new[rand_val_idx[i*n_col+j]], 'r')\n",
    "        axarr[i, j].plot(test_spks_new[rand_val_idx[i*n_col+j]], 'b')\n",
    "        axarr[i, j].set_ylim([ymin*1.1, ymax*1.1])\n",
    "        axarr[i, j].axis('off')\n",
    "#plt.show()\n",
    "#plt.savefig('waveclus_spks.eps', format='eps', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
