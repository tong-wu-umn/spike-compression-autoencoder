{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import line_profiler\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, BatchSampler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from model.model_v2 import spk_vq_vae_resnet\n",
    "from model.utils import SpikeDataset\n",
    "\n",
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% global parameters\n",
    "spk_ch = 4\n",
    "spk_dim = 64 # for Wave_Clus\n",
    "# spk_dim = 48 # for HC1 and Neuropixels\n",
    "log_interval = 10\n",
    "beta = 0.15\n",
    "vq_num = 128\n",
    "batch_size = 48\n",
    "test_batch_size = 1000\n",
    "\n",
    "\"\"\"\n",
    "org_dim     = param[0]\n",
    "conv1_ch    = param[1]\n",
    "conv2_ch    = param[2]\n",
    "conv0_ker   = param[3]\n",
    "conv1_ker   = param[4]\n",
    "conv2_ker   = param[5]\n",
    "self.vq_dim = param[6]\n",
    "self.vq_num = param[7]\n",
    "cardinality = param[8]\n",
    "dropRate    = param[9]\n",
    "\"\"\"\n",
    "param_resnet_v2 = [spk_ch, 256, 16, 1, 3, 1, int(spk_dim/4), vq_num, 32, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_file = './data/noisy_spks.mat'\n",
    "clean_file = './data/clean_spks.mat'\n",
    "\n",
    "args = collections.namedtuple\n",
    "\n",
    "# training set purposely distorted to train denoising autoencoder\n",
    "args.data_path = noise_file\n",
    "args.train_portion = .5\n",
    "args.train_mode = True\n",
    "train_noise = SpikeDataset(args)\n",
    "\n",
    "# clean dataset for training\n",
    "args.data_path = clean_file\n",
    "args.train_portion = .5\n",
    "args.train_mode = True\n",
    "train_clean = SpikeDataset(args)\n",
    "\n",
    "# noisy datast for training\n",
    "args.data_path = noise_file\n",
    "args.train_portion = .5\n",
    "args.train_mode = False\n",
    "test_noise = SpikeDataset(args)\n",
    "\n",
    "# clean dataset for testing\n",
    "args.data_path = clean_file\n",
    "args.train_portion = .5\n",
    "args.train_mode = False\n",
    "test_clean = SpikeDataset(args)\n",
    "\n",
    "batch_cnt = int(math.ceil(len(train_noise) / batch_size))\n",
    "\n",
    "# normalization\n",
    "d_mean, d_std = train_clean.get_normalizer()\n",
    "\n",
    "train_clean.apply_norm(d_mean, d_std)\n",
    "train_noise.apply_norm(d_mean, d_std)\n",
    "test_clean.apply_norm(d_mean, d_std)\n",
    "test_noise.apply_norm(d_mean, d_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% create model\n",
    "model = spk_vq_vae_resnet(param_resnet_v2).to(gpu)\n",
    "\n",
    "# %% loss and optimization function\n",
    "def loss_function(recon_x, x, commit_loss, vq_loss):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    return recon_loss + beta * commit_loss + vq_loss, recon_loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    batch_sampler = BatchSampler(RandomSampler(range(len(train_noise))), batch_size=batch_size, drop_last=False)\n",
    "    for batch_idx, ind in enumerate(batch_sampler):\n",
    "        in_data = train_noise[ind].to(gpu)\n",
    "        out_data = train_clean[ind].to(gpu)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, commit_loss, vq_loss = model(in_data)\n",
    "        loss, recon_loss = loss_function(recon_batch, out_data, commit_loss, vq_loss)\n",
    "        loss.backward(retain_graph=True)\n",
    "        model.bwd()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += recon_loss.item() / (spk_dim * spk_ch)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(in_data), len(train_noise),\n",
    "                100. * batch_idx / batch_cnt, recon_loss.item()))\n",
    "    \n",
    "    average_train_loss = train_loss / len(train_noise)\n",
    "    print('====> Epoch: {} Average train loss: {:.5f}'.format(\n",
    "          epoch, average_train_loss))\n",
    "    return average_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model logging\n",
    "best_val_loss = 10\n",
    "cur_train_loss = 1\n",
    "def save_model(val_loss, train_loss):\n",
    "\tglobal best_val_loss, cur_train_loss\n",
    "\tif val_loss < best_val_loss:\n",
    "\t\tbest_val_loss = val_loss\n",
    "\t\tcur_train_loss = train_loss\n",
    "\t\ttorch.save(model.state_dict(), './spk_vq_vae_temp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, test_mode=True):\n",
    "    if test_mode:\n",
    "        model.eval()\n",
    "    model.embed_reset()\n",
    "    test_loss = 0\n",
    "    recon_sig = torch.rand(1, spk_ch, spk_dim)\n",
    "    org_sig = torch.rand(1, spk_ch, spk_dim)\n",
    "    with torch.no_grad():\n",
    "        batch_sampler = BatchSampler(RandomSampler(range(len(test_noise))), batch_size=test_batch_size, drop_last=False)\n",
    "        for batch_idx, ind in enumerate(batch_sampler):\n",
    "            in_data = test_noise[ind].to(gpu)\n",
    "            out_data = test_clean[ind].to(gpu)\n",
    "\n",
    "            recon_batch, commit_loss, vq_loss = model(in_data)\n",
    "            _, recon_loss = loss_function(recon_batch, out_data, commit_loss, vq_loss)\n",
    "        \n",
    "            recon_sig = torch.cat((recon_sig, recon_batch.data.cpu()), dim=0)\n",
    "            org_sig = torch.cat((org_sig, out_data.data.cpu()), dim=0)\n",
    "        \n",
    "            test_loss += recon_loss.item() / (spk_dim * spk_ch)\n",
    "\n",
    "        average_test_loss = test_loss / len(test_noise)\n",
    "        print('====> Epoch: {} Average test loss: {:.5f}'.format(\n",
    "              epoch, average_test_loss))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        plt.figure(figsize=(7,5))\n",
    "        plt.bar(np.arange(vq_num), model.embed_freq / model.embed_freq.sum())\n",
    "        plt.ylabel('Probability of Activation', fontsize=16)\n",
    "        plt.xlabel('Index of codewords', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    return average_test_loss, recon_sig[1:], org_sig[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "epochs = 500\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss, _, _ = test(epoch)\n",
    "    save_model(test_loss, train_loss)\n",
    "    \n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Minimal train/testing losses are {:.4f} and {:.4f} with index {}\\n'\n",
    "    .format(cur_train_loss, best_val_loss, test_loss_history.index(min(test_loss_history))))\n",
    "\n",
    "# plot train and test loss history over epochs\n",
    "plt.figure(1)\n",
    "epoch_axis = range(1, len(train_loss_history) + 1)\n",
    "plt.plot(epoch_axis, train_loss_history, 'bo')\n",
    "plt.plot(epoch_axis, test_loss_history, 'b+')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Visualization of mostly used VQ vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best performing model\n",
    "model.load_state_dict(torch.load('./spk_vq_vae_temp.pt'))\n",
    "\n",
    "embed_idx = np.argsort(model.embed_freq)\n",
    "embed_sort = model.embed.weight.data.cpu().numpy()[embed_idx]\n",
    "\n",
    "# Visualizing activation pattern of VQ codes on testing dataset (the first 8 mostly activated)\n",
    "plt.figure()\n",
    "n_row, n_col = 1, 8\n",
    "f, axarr = plt.subplots(n_row, n_col, figsize=(n_col*2, n_row*2))\n",
    "for i in range(8):\n",
    "    axarr[i].plot(embed_sort[i], 'r')\n",
    "    axarr[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% spike recon\n",
    "train_mean, train_std = torch.from_numpy(d_mean), torch.from_numpy(d_std)\n",
    "_, val_spks, test_spks = test(10)\n",
    "\n",
    "# calculate compression ratio\n",
    "vq_freq = model.embed_freq / sum(model.embed_freq)\n",
    "vq_freq = vq_freq[vq_freq != 0]\n",
    "vq_log2 = np.log2(vq_freq)\n",
    "bits = -sum(np.multiply(vq_freq, vq_log2))\n",
    "cr = spk_ch * spk_dim * 16 / (param_resnet_v2[2] * bits)\n",
    "print('compression ratio is {:.2f} with {:.2f}-bit.'.format(cr, bits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. MSE error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recon_spks = val_spks * train_std + train_mean\n",
    "test_spks_v2 = test_spks * train_std + train_mean\n",
    "\n",
    "recon_spks = recon_spks.view(-1, spk_dim)\n",
    "test_spks_v2 = test_spks_v2.view(-1, spk_dim)\n",
    "\n",
    "recon_err = torch.norm(recon_spks-test_spks_v2, p=2, dim=1) / torch.norm(test_spks_v2, p=2, dim=1)\n",
    "\n",
    "print('mean of recon_err is {:.4f}'.format(torch.mean(recon_err)))\n",
    "print('std of recon_err is {:.4f}'.format(torch.std(recon_err)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. SNDR of reconstructed spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_spks_new = recon_spks.numpy()\n",
    "test_spks_new = test_spks_v2.numpy()\n",
    "\n",
    "def cal_sndr(org_data, recon_data):\n",
    "    org_norm = np.linalg.norm(org_data, axis=1)\n",
    "    err_norm = np.linalg.norm(org_data-recon_data, axis=1)\n",
    "    return np.mean(20*np.log10(org_norm / err_norm)), np.std(20*np.log10(org_norm / err_norm))\n",
    "\n",
    "cur_sndr, sndr_std = cal_sndr(test_spks_new, recon_spks_new)\n",
    "print('SNDR is {:.4f} with std {:.4f}'.format(cur_sndr, sndr_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Visualization of reconstructed spikes chosen at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_val_idx = np.random.permutation(len(recon_spks_new))\n",
    "\n",
    "plt.figure()\n",
    "n_row, n_col = 3, 8\n",
    "spks_to_show = test_spks_new[rand_val_idx[:n_row*n_col]]\n",
    "ymax, ymin = np.amax(spks_to_show), np.amin(spks_to_show)\n",
    "f, axarr = plt.subplots(n_row, n_col, figsize=(n_col*3, n_row*3))\n",
    "for i in range(n_row):\n",
    "    for j in range(n_col):\n",
    "        axarr[i, j].plot(recon_spks_new[rand_val_idx[i*n_col+j]], 'r')\n",
    "        axarr[i, j].plot(test_spks_new[rand_val_idx[i*n_col+j]], 'b')\n",
    "        axarr[i, j].set_ylim([ymin*1.1, ymax*1.1])\n",
    "        axarr[i, j].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
